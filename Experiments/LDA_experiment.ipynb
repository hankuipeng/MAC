{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Actiontec - MyWirelessTV2 Wireless Video Trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Turtle Beach - Ear Force XO SEVEN PRO Gaming H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Sony - Passive 3D Glasses - Black\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Asus - WirelessAC1900 Dual-Band Gigabit Wirele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sony - AM/FM Dual-Alarm Clock Radio - Black\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Actiontec - MyWirelessTV2 Wireless Video Trans...\n",
       "1  Turtle Beach - Ear Force XO SEVEN PRO Gaming H...\n",
       "2                 Sony - Passive 3D Glasses - Black\"\n",
       "3  Asus - WirelessAC1900 Dual-Band Gigabit Wirele...\n",
       "4       Sony - AM/FM Dual-Alarm Clock Radio - Black\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the text data\n",
    "import pandas as pd\n",
    "raw_text = pd.read_csv('Data/product_names.csv', header = None)\n",
    "raw_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the length of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2921"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hankui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import re \n",
    "import string\n",
    "import csv \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function that removes numbers and special characters from product \n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def text_process(mess):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all numbers and punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove the numbers in the character string\n",
    "    mess = re.sub('[0-9]+', '', mess)\n",
    "    \n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word.lower() for word in nopunc.split() if word.lower() not in stopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['actiontec',\n",
       "  'mywirelesstv',\n",
       "  'wireless',\n",
       "  'video',\n",
       "  'transmitter',\n",
       "  'receiver',\n",
       "  'black'],\n",
       " ['turtle',\n",
       "  'beach',\n",
       "  'ear',\n",
       "  'force',\n",
       "  'xo',\n",
       "  'seven',\n",
       "  'pro',\n",
       "  'gaming',\n",
       "  'headset',\n",
       "  'xbox',\n",
       "  'one',\n",
       "  'blackgreen'],\n",
       " ['sony', 'passive', 'glasses', 'black'],\n",
       " ['asus', 'wirelessac', 'dualband', 'gigabit', 'wireless', 'router', 'black'],\n",
       " ['sony', 'amfm', 'dualalarm', 'clock', 'radio', 'black']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess the text data with the function above\n",
    "names = raw_text.iloc[:,0].to_numpy() \n",
    "\n",
    "N = len(names) # the number of product names\n",
    "\n",
    "proc_text = [text_process(names[i]) for i in range(N)]\n",
    "\n",
    "proc_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hankui/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 actiontec\n",
      "1 black\n",
      "2 mywirelesstv\n",
      "3 receiver\n",
      "4 transmitter\n",
      "5 video\n",
      "6 wireless\n",
      "7 beach\n",
      "8 blackgreen\n",
      "9 ear\n",
      "10 force\n"
     ]
    }
   ],
   "source": [
    "# check the unique words in the dictionary\n",
    "dictionary = gensim.corpora.Dictionary(proc_text)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(31, 1), (32, 1), (33, 1), (34, 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the bag of words corpus\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in proc_text]\n",
    "bow_corpus[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the LDA model \n",
    "K = 5\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = K, id2word = dictionary, passes = 2, workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.041*\"black\" + 0.019*\"wireless\" + 0.012*\"oz\"'), (1, '0.023*\"black\" + 0.015*\"oz\" + 0.010*\"wireless\"'), (2, '0.023*\"black\" + 0.020*\"amp\" + 0.012*\"drive\"'), (3, '0.019*\"black\" + 0.013*\"white\" + 0.011*\"oz\"'), (4, '0.023*\"wireless\" + 0.023*\"oz\" + 0.019*\"white\"')]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.print_topics(num_topics = K, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the LDA labels\n",
    "grps_lda = []\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    values = [v[1] for v in lda_model[bow_corpus[i]]]\n",
    "\n",
    "    grps_lda.append(max(range(len(values)), key=values.__getitem__) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 1, 1, 5]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grps_lda[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  2\n",
       "1  4\n",
       "2  1\n",
       "3  1\n",
       "4  5"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the labels to a dataframe first (for using to_csv function in the next step)\n",
    "grps_lda = pd.DataFrame(grps_lda)\n",
    "\n",
    "grps_lda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the numerical labels to a csv file \n",
    "grps_lda.to_csv('grps_lda.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the above code to check for running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02.154230\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "# check the unique words in the dictionary\n",
    "dictionary = gensim.corpora.Dictionary(proc_text)\n",
    "\n",
    "# create the bag of words corpus\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in proc_text]\n",
    "\n",
    "# run the LDA model \n",
    "K = 5\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = K, id2word = dictionary, passes = 2, workers = 2)\n",
    "\n",
    "# get the LDA labels\n",
    "grps_lda = []\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    values = [v[1] for v in lda_model[bow_corpus[i]]]\n",
    "\n",
    "    grps_lda.append(max(range(len(values)), key=values.__getitem__) + 1)\n",
    "\n",
    "print(datetime.datetime.now() - begin_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
